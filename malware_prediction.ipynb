{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTKdTedWZD3P"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import sklearn.metrics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qK5_1e3Su0aq"
   },
   "outputs": [],
   "source": [
    "#特徴量ごとの個別の処理\n",
    "def  preprocess_for_features(datas):\n",
    "    #Noneを平均値239142.39517997563に変換し、値を1000で除算して小数点以下を四捨五入する（後にtarget encodingを実施）\n",
    "    datas['Census_OEMModelIdentifier'] = datas['Census_OEMModelIdentifier'].astype(np.float64).fillna(239142.39517997563).map(lambda x: round(x / 1000))\n",
    "    \n",
    "    #Noneを中央値28542.0に変換し、10で除算して小数点以下を四捨五入する（後にtarget encodingを実施）\n",
    "    datas['Census_SystemVolumeTotalCapacity'] = datas['Census_SystemVolumeTotalCapacity'].astype(np.float64).fillna(28542.0).map(lambda x: round(x / 10))\n",
    "    \n",
    "    #Noneを0.0に変換する\n",
    "    datas['IsProtected'] = datas['IsProtected'].astype(np.float64).fillna(0.0)\n",
    "    \n",
    "    #OsVerとPlatformを連結（同じPlatformでもOsのバージョンによってウィルス検出率に違いがあるため）\n",
    "    datas['OsVer*Platform'] = datas['OsVer'].fillna('x') + datas['Platform'].fillna('x')\n",
    "    \n",
    "    #Census_InternalBatteryTypeの有効な値以外は一律で'z'に変換\n",
    "    datas['Census_InternalBatteryType'] = datas['Census_InternalBatteryType'].map(lambda x: 'z' if x not in('#','4cel','bad','batt','bq20','ca48','ithi','lgi0','lhp0','li','li p','li-i','liio','lion','lip','lipo','lipp','lit','nimh','pbac','ram','real','unkn','vbox','virt') else x )\n",
    "    \n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrJ_Yz_rX9vE"
   },
   "outputs": [],
   "source": [
    "#訓練データの各カラムの合計を算出(平均値の算出に使用)\n",
    "def get_sum(original_csv, conversion_dict, ohe_columns, te_columns, train_df):\n",
    "    \n",
    "    #特徴量ごとの個別の処理を生のデータに適用\n",
    "    original_csv = preprocess_for_features(original_csv)\n",
    "\n",
    "    #カテゴリ変数のNoneを'x'に置換（target encodingによる変換用のdictionary内のNULL値と一致させるため）\n",
    "    original_csv[ohe_columns] = original_csv[ohe_columns].fillna('x')\n",
    "    original_csv[te_columns] = original_csv[te_columns].fillna('x')\n",
    "    \n",
    "    #Ohe Hot Encoding（train data全体でone hot encodingした場合の結果と合致させる）\n",
    "    ohe_csv = pd.get_dummies(original_csv, dummy_na=True, columns=ohe_columns)\n",
    "    train_df_columns = list(set(train_df.columns.values) - set(ohe_csv.columns.values))\n",
    "    for column in train_df_columns:\n",
    "        ohe_csv[column] = 0.0\n",
    "    ohe_csv = ohe_csv[train_df.columns]\n",
    "    \n",
    "    #Target Encoding（target encodingによる変換用のdictionaryを用いて変換）\n",
    "    te_csv = ohe_csv.copy()\n",
    "    for column in te_columns:\n",
    "        te_csv[column] = te_csv[column].map(conversion_dict[column])\n",
    "\n",
    "    #データ全体での各カラムの平均値を算出するため、合計値を算出\n",
    "    return te_csv.astype(np.float64).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wbq3nRHbXqxh"
   },
   "outputs": [],
   "source": [
    "#訓練データをfit\n",
    "def train(num, original_csv, conversion_dict, ohe_columns, te_columns, train_df, mean_dict):\n",
    "    \n",
    "    #特徴量ごとの個別の処理を生のデータに適用\n",
    "    original_csv = preprocess_for_features(original_csv)\n",
    "    \n",
    "    IDs = original_csv['MachineIdentifier']\n",
    "    y = original_csv['HasDetections'].astype(np.float64)\n",
    "    X = original_csv.drop('MachineIdentifier', axis=1).drop('HasDetections', axis=1)\n",
    "    \n",
    "    #カテゴリ変数のNoneを'x'に置換（target encodingによる変換用のdictionary内のNULL値と一致させるため）\n",
    "    X[ohe_columns] = X[ohe_columns].fillna('x')\n",
    "    X[te_columns] = X[te_columns].fillna('x')\n",
    "    \n",
    "    #Ohe Hot Encoding（train data全体でone hot encodingした場合の結果と合致させる）\n",
    "    ohe_X = pd.get_dummies(X, dummy_na=True, columns=ohe_columns)\n",
    "    train_df_columns = list(set(train_df.columns.values) - set(ohe_X.columns.values))\n",
    "    for column in train_df_columns:\n",
    "        ohe_X[column] = 0.0\n",
    "    ohe_X = ohe_X[train_df.columns]\n",
    "    \n",
    "    #Target Encoding（target encodingによる変換用のdictionaryを用いて変換）\n",
    "    te_X = ohe_X.copy()\n",
    "    for column in te_columns:\n",
    "        te_X[column] = te_X[column].map(conversion_dict[column])\n",
    "    \n",
    "    #各カラム平均値の対応を保持するdictionaryを用いてNoneを平均値に置換\n",
    "    for mean in mean_dict.items():\n",
    "        te_X[mean[0]] = te_X[mean[0]].fillna(mean[1])\n",
    "    \n",
    "    #modelにfitするため全てのデータ型をfloatに変換\n",
    "    te_X = te_X.astype(np.float64)\n",
    "    \n",
    "    #modelをロード\n",
    "    with open('xgb_model.pickle', mode='rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    #分割した訓練データの内の一つ目なら、modelを初期化\n",
    "    if num == 1:\n",
    "        model = None\n",
    "    \n",
    "    #5000行ずつmodelを訓練\n",
    "    batch_size = 5000\n",
    "    iterations = 1\n",
    "    for i in range(iterations):\n",
    "        for start in range(0, len(te_X), batch_size):\n",
    "            model = xgb.train({\n",
    "                'binary':'hinge', \n",
    "                'learning_rate': 0.007,\n",
    "                'eval_metric': 'auc', \n",
    "                'update':'refresh',\n",
    "                'refresh_leaf': True,\n",
    "                'reg_alpha': 3, \n",
    "                'silent': False,\n",
    "            }, dtrain=xgb.DMatrix(te_X[start:start+batch_size], y[start:start+batch_size]), xgb_model=model)\n",
    "\n",
    "    #modelを保存\n",
    "    with open('xgb_model.pickle', mode='wb') as fp:\n",
    "        pickle.dump(model, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjNPQt_iXqxm"
   },
   "outputs": [],
   "source": [
    "def test(num, original_csv, conversion_dict, ohe_columns, te_columns, train_df, mean_dict):\n",
    "    print('start ' + str(num))\n",
    "    \n",
    "    #特徴量ごとの個別の処理を生のデータに適用\n",
    "    original_csv = preprocess_for_features(original_csv)\n",
    "    \n",
    "    IDs = original_csv['MachineIdentifier']\n",
    "    X = original_csv.drop('MachineIdentifier', axis=1)\n",
    "    \n",
    "    #カテゴリ変数のNoneを'x'に置換（target encodingによる変換用のdictionary内のNULL値と一致させるため）\n",
    "    X[ohe_columns] = X[ohe_columns].fillna('x')\n",
    "    X[te_columns] = X[te_columns].fillna('x')\n",
    "    \n",
    "    #Ohe Hot Encoding（train data全体でone hot encodingした場合の結果と合致させる）\n",
    "    ohe_X = pd.get_dummies(X, dummy_na=True, columns=ohe_columns)\n",
    "    train_df_columns = list(set(train_df.columns.values) - set(ohe_X.columns.values))\n",
    "    for column in train_df_columns:\n",
    "        ohe_X[column] = 0.0\n",
    "    ohe_X = ohe_X[train_df.columns]\n",
    "    \n",
    "    #Target Encoding（target encodingによる変換用のdictionaryを用いて変換）\n",
    "    te_X = ohe_X.copy()\n",
    "    for column in te_columns:\n",
    "        te_X[column] = te_X[column].map(conversion_dict[column])\n",
    "    \n",
    "    #各カラム平均値の対応を保持するdictionaryを用いてNoneを平均値に置換\n",
    "    imputed_X = te_X.copy()\n",
    "    for mean in mean_dict.items():\n",
    "        imputed_X[mean[0]] = te_X[mean[0]].fillna(mean[1])\n",
    "    \n",
    "    #modelでpredictするため全てのデータ型をfloatに変換\n",
    "    imputed_X = imputed_X.astype(np.float64)\n",
    "    \n",
    "    #modelをロード\n",
    "    with open('xgb_model.pickle', mode='rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    #predict\n",
    "    imputed_X['predictions'] = model.predict(xgb.DMatrix(imputed_X))\n",
    "    imputed_X['MachineIdentifier'] = IDs\n",
    "    return imputed_X[['MachineIdentifier', 'predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JFM67LKUXqxq"
   },
   "outputs": [],
   "source": [
    "#各カラムの一意の値ごとの数と、その値の中でHasDetectionsが1である数を保有するcsvを読み込む\n",
    "#これらは予めデータベース内でテーブル上に作成し、csvファイルとして出力した\n",
    "value_ratio = pd.read_csv('value_ratio2.csv', dtype=np.object, engine='python')\n",
    "\n",
    "#target encodingまたはcategory encodingを実施する際に、None値をxとして統一する\n",
    "#（「Nan」や「None」などの微妙な表記の違いによる変換ミスを回避するため）\n",
    "value_ratio['value_name'] = value_ratio['value_name'].fillna('x')\n",
    "unique_values = {}\n",
    "\n",
    "#カテゴリ変数を定義\n",
    "categorical_columns = ['OsVer*Platform', 'Census_SystemVolumeTotalCapacity', 'ProductName', 'EngineVersion', 'AppVersion', 'AvSigVersion', 'RtpStateBitfield', 'DefaultBrowsersIdentifier', 'AVProductStatesIdentifier', 'CountryIdentifier', 'CityIdentifier', 'OrganizationIdentifier', 'GeoNameIdentifier', 'LocaleEnglishNameIdentifier', 'Processor', 'OsSuite', 'OsPlatformSubRelease', 'OsBuildLab', 'SkuEdition', 'PuaMode', 'IeVerIdentifier', 'SmartScreen', 'Census_MDC2FormFactor', 'Census_DeviceFamily', 'Census_OEMNameIdentifier', 'Census_OEMModelIdentifier', 'Census_ProcessorManufacturerIdentifier', 'Census_ProcessorModelIdentifier', 'Census_ProcessorClass', 'Census_PrimaryDiskTypeName', 'Census_ChassisTypeName', 'Census_InternalPrimaryDisplayResolutionHorizontal', 'Census_InternalPrimaryDisplayResolutionVertical', 'Census_PowerPlatformRoleName', 'Census_InternalBatteryType', 'Census_OSVersion', 'Census_OSBranch', 'Census_OSBuildNumber', 'Census_OSEdition', 'Census_OSInstallTypeName', 'Census_OSInstallLanguageIdentifier', 'Census_OSWUAutoUpdateOptionsName', 'Census_GenuineStateName', 'Census_ActivationChannel', 'Census_IsFlightingInternal', 'Census_IsFlightsDisabled', 'Census_FlightRing', 'Census_ThresholdOptIn', 'Census_FirmwareManufacturerIdentifier', 'Census_FirmwareVersionIdentifier', 'Census_IsWIMBootEnabled', 'Census_IsVirtualDevice', 'Census_IsAlwaysOnAlwaysConnectedCapable', 'Wdft_IsGamer', 'Wdft_RegionIdentifier']\n",
    "\n",
    "#各種encodingの対象カラムと、全encoding実施後のDataFrameのプロトタイプを定義\n",
    "ohe_columns = []\n",
    "te_columns = []\n",
    "train_df = pd.read_csv('train.csv', nrows=0, dtype=np.float64, engine='python').drop('MachineIdentifier', axis=1).drop('HasDetections', axis=1)\n",
    "train_df['OsVer*Platform'] = None\n",
    "\n",
    "#不要なカラムをドロップ\n",
    "train_df = train_df.drop('UacLuaenable', axis=1)\n",
    "train_df = train_df.drop('Census_OSArchitecture', axis=1)\n",
    "train_df = train_df.drop('Census_OSSkuName', axis=1)\n",
    "train_df = train_df.drop('Census_OSUILocaleIdentifier', axis=1)\n",
    "train_df = train_df.drop('OsBuild', axis=1)\n",
    "train_df = train_df.drop('OsVer', axis=1)\n",
    "train_df = train_df.drop('Platform', axis=1)\n",
    "\n",
    "#カテゴリ変数を一つずつ処理\n",
    "for column in categorical_columns:\n",
    "    column_values = {}\n",
    "    #対象カラムの一意な値の数が10個以下ならone hot encoding対象カラムのリストに追加し、\n",
    "    #train_dfをそのカラムにone hot encodingを施した後の状態にする\n",
    "    if len(value_ratio[value_ratio['column_name']==column]) < 11:\n",
    "        ohe_columns.append(column)\n",
    "        for _, value in value_ratio[value_ratio['column_name']==column].iterrows():\n",
    "            train_df[column + '_' + str(value['value_name'])] = None\n",
    "        train_df = train_df.drop(column, axis=1)\n",
    "    #対象カラムの一意な値の数が11個以上ならtarget encoding対象カラムのリストに追加し、\n",
    "    #カラム内のそれぞれの値との値の中でHasDetectionsが1である割合を計算する。\n",
    "    #（その際に「smoothing」の手法を用いる）\n",
    "    #最終的に一意の値とそれぞれの変換後の数値の組を保有するdictionaryを作成する。\n",
    "    else:\n",
    "        te_columns.append(column)\n",
    "        all_count = value_ratio[value_ratio['column_name']==column]['value_count'].astype(np.float64).sum()\n",
    "        all_detected_count = value_ratio[value_ratio['column_name']==column]['detected_count'].astype(np.float64).sum()\n",
    "        value_ratio['value_count'] = value_ratio['value_count'].astype(np.float64)\n",
    "        value_ratio['detected_count'] = value_ratio['detected_count'].astype(np.float64)\n",
    "        for _, value in value_ratio[value_ratio['column_name']==column].iterrows():\n",
    "            lm = 1 / (1 + math.exp(-value['value_count']))\n",
    "            ratio = lm * value['detected_count'] / value['value_count'] + (1 - lm) * all_detected_count / all_count\n",
    "            column_values[value['value_name']] = ratio\n",
    "    unique_values[column] = column_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 848908,
     "status": "ok",
     "timestamp": 1552207448836,
     "user": {
      "displayName": "藤井隆史",
      "photoUrl": "",
      "userId": "02292301144144879183"
     },
     "user_tz": -540
    },
    "id": "hIAKYtEsXqxs",
    "outputId": "2f93e1a5-c1da-4f51-835b-b00024d0b397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "#各カラムの欠測値を平均で補完するため、前処理完了後の全訓練データの全カラムの平均を取得する。\n",
    "#その際、全800万件の平均を一度に計算することはメモリが足りずできなかったため、\n",
    "#50万件ずつ各カラムの合計値を算出していき、最終的にそれらの総和をデータ件数で除算することで平均値を算出。\n",
    "#最終的に前処理後のカラムとそれぞれの平均値の組を保有するdictionaryを作成する。\n",
    "\n",
    "amount = pd.Series()\n",
    "original_csv_chunk =  pd.read_csv('train.csv', dtype=np.object, engine='python', chunksize=500000)\n",
    "counter=1\n",
    "for original_csv in original_csv_chunk:\n",
    "    print(counter)\n",
    "    if counter == 1:\n",
    "    amount = get_sum(original_csv, unique_values, ohe_columns, te_columns, train_df)\n",
    "    else:\n",
    "        amount = amount + get_sum(original_csv, unique_values, ohe_columns, te_columns, train_df)\n",
    "    counter += 1\n",
    "#合計を訓練データ数で割り、各カラムの平均を算出\n",
    "mean_dict = dict(amount / 8921483)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "#全8921483件の訓練データを訓練データを50万件ずつオンライン学習\n",
    "original_csv_chunk =  pd.read_csv('train.csv', dtype=np.object, engine='python', chunksize=500000)\n",
    "counter=1\n",
    "for original_csv in original_csv_chunk:\n",
    "    print(counter)\n",
    "    train(counter, original_csv, unique_values, ohe_columns, te_columns, train_df, mean_dict)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9783445,
     "status": "ok",
     "timestamp": 1552233888867,
     "user": {
      "displayName": "藤井隆史",
      "photoUrl": "",
      "userId": "02292301144144879183"
     },
     "user_tz": -540
    },
    "id": "WbzLdsF0tmOY",
    "outputId": "0f7bef94-8953-450f-b06a-14274b31b80d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 1\n",
      "1th was done\n",
      "start 2\n",
      "2th was done\n",
      "start 3\n",
      "3th was done\n",
      "start 4\n",
      "4th was done\n",
      "start 5\n",
      "5th was done\n",
      "start 6\n",
      "6th was done\n",
      "start 7\n",
      "7th was done\n",
      "start 8\n",
      "8th was done\n",
      "start 9\n",
      "9th was done\n",
      "start 10\n",
      "10th was done\n",
      "start 11\n",
      "11th was done\n",
      "start 12\n",
      "12th was done\n",
      "start 13\n",
      "13th was done\n",
      "start 14\n",
      "14th was done\n",
      "start 15\n",
      "15th was done\n",
      "start 16\n",
      "16th was done\n"
     ]
    }
   ],
   "source": [
    "#全7853253件のテストデータを50万件ずつpredictし、csvに追記\n",
    "original_csv_chunk =  pd.read_csv('test.csv', dtype=np.object, engine='python', chunksize=500000)\n",
    "counter=1\n",
    "for original_csv in original_csv_chunk:\n",
    "    if counter == 1:\n",
    "        test(counter, original_csv, unique_values, ohe_columns, te_columns, train_df, mean_dict).to_csv('result.csv', index=False)\n",
    "    else:\n",
    "        test(counter, original_csv, unique_values, ohe_columns, te_columns, train_df, mean_dict).to_csv('result.csv', index=False, header=None, mode='a')\n",
    "    print(str(counter) + 'th was done')\n",
    "    counter += 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "search2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
